{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d869eae7",
   "metadata": {},
   "source": [
    "## What is Textblob\n",
    "\n",
    "* Textblob is an open-source python library used to perform NLP activities like Lemmatization, Stemming, Tokenization, Noun Phrase Extraction, POS Tagging, N-Grams, Sentiment Analysis. \n",
    "\n",
    "* It is faster than NLTK, however it does not provide the functionalities like vectorization, dependency parsing.\n",
    "\n",
    "* Text Classification, Sentiment Analysis can be performed using Textblob. \n",
    "* Official Link to Textblob is: https://textblob.readthedocs.io/en/dev/\n",
    "\n",
    "* Installation: pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ea0c543",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\anusha\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: textblob in c:\\users\\anusha\\anaconda3\\lib\\site-packages (0.18.0.post0)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\anusha\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "### Install Textblob\n",
    "!pip install nltk\n",
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51aa56b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5213c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anusha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a807ca3",
   "metadata": {},
   "source": [
    "### Functionalities of Textblob\n",
    "* Language Detection\n",
    "* Word Correction\n",
    "* Word Count\n",
    "* Phrase Extraction\n",
    "* POS Tagging\n",
    "* Tokenization\n",
    "* Plularization of words using Textblob\n",
    "* Lemmatization using Textblob\n",
    "* n-gram in Textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059517a",
   "metadata": {},
   "source": [
    "#### Language Detection\n",
    "* With the help of Google Translate, Textblob detects the language of input text. \n",
    "* Textblob is also able to translate text from one language to another language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9fdb42d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextBlob' object has no attribute 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m blob \u001b[38;5;241m=\u001b[39m TextBlob(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHey John, How are You\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#print(\"Detected Language is:\",blob.detect_language())\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput text in Spanish:\u001b[39m\u001b[38;5;124m\"\u001b[39m,blob\u001b[38;5;241m.\u001b[39mtranslate(to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mes\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextBlob' object has no attribute 'translate'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    " \n",
    "blob = TextBlob(\"Hey John, How are You\")\n",
    " \n",
    "print(\"Detected Language is:\",blob.detect_language())\n",
    " \n",
    "print(\"Input text in Spanish:\",blob.translate(to='es'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8710e0b7",
   "metadata": {},
   "source": [
    "### Note:\n",
    "Since Google has made some changes into its API and Textblob is using the older API, as a result you may get 404 error. To avoide this, change the url given in translate.py under your environment. \n",
    "\n",
    "\n",
    "updated url link is:\n",
    "url = \"http://translate.google.com/translate_a/t?client=te&format=html&dt=bd&dt=ex&dt=ld&dt=md&dt=qca&dt=rw&dt=rm&dt=ss&dt=t&dt=at&ie=UTF-8&oe=UTF-8&otf=2&ssel=0&tsel=0&kc=1\"\n",
    "\n",
    "\n",
    "Location of translate.py file: C:\\Users\\<user name>\\Anaconda3\\envs\\Rython\\Lib\\site-packages\\textblob\\translate.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fe5f92",
   "metadata": {},
   "source": [
    "#### Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adf80d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "text=\"\"\" ABCD Corp alays values ttheir employees!!!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0757e15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ABCD Corp alays values ttheir employees!!!\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe50b2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bddbd582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\" ABCD Corp alays values ttheir employees!!!\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0941e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\" ABCD For always values their employees!!!\")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29eab446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"has\")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob('hasss').correct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "049a9342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"or\")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Sometimes it failsas well\n",
    "TextBlob('ur').correct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe3704f",
   "metadata": {},
   "source": [
    "### Word Count \n",
    "With the help of word count, we can count the frequency of words or a noun phrase in a given sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1169c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Sentiment Analysis is a process by which we can find the sentiment of a text. Sentiment can be Positive, Negative or Neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad31d646",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b830875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"analysis\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9daa2972",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"Sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fbf1a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a39c4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.word_counts[\"Analysis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3582855",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "With the help of tags function of textblob, we can get tag each words of a sentence with a tag that can be either noun, pronoun, verb, adverb, adjective and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "680e50c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('My', 'PRP$'), ('name', 'NN'), ('is', 'VBZ'), ('Adam', 'NNP'), ('I', 'PRP'), ('like', 'VBP'), ('to', 'TO'), ('read', 'VB'), ('about', 'IN'), ('NLP', 'NNP'), ('I', 'PRP'), ('work', 'VBP'), ('at', 'IN'), ('ABCD', 'NNP'), ('Corp', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    " \n",
    "text = TextBlob(\"My name is Adam. I like to read about NLP. I work at ABCD Corp.\")\n",
    "print(text.tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a7c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('My', 'PRP$')\n",
      "('name', 'NN')\n",
      "('is', 'VBZ')\n",
      "('Adam', 'NNP')\n",
      "('I', 'PRP')\n",
      "('like', 'VBP')\n",
      "('to', 'TO')\n",
      "('read', 'VB')\n",
      "('about', 'IN')\n",
      "('NLP', 'NNP')\n",
      "('I', 'PRP')\n",
      "('work', 'VBP')\n",
      "('at', 'IN')\n",
      "('ABCD', 'NNP')\n",
      "('Corp', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "new_tuple=[]\n",
    "for i in text.tags:\n",
    "    print(i)\n",
    "    if 'VBP' not in i[1]:\n",
    "        new_tuple.append(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f2a4260c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('My', 'PRP$'),\n",
       " ('name', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('Adam', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('to', 'TO'),\n",
       " ('read', 'VB'),\n",
       " ('about', 'IN'),\n",
       " ('NLP', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('at', 'IN'),\n",
       " ('ABCD', 'NNP'),\n",
       " ('Corp', 'NNP')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2be7d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "value=''\n",
    "for i in new_tuple:\n",
    "    value=value+\" \" + \"\".join(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d3a2219",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My name is Adam I to read about NLP I at ABCD Corp'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570dce4",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee75c7a",
   "metadata": {},
   "source": [
    "* Corpus (or corpora in plural) - Corpus is nothing but a collection of text data. The text maybe in one language or maybe a combination of two or more. \n",
    "\n",
    "* Token - The term \"Token\" is nothing but the total number of words in a text, corpus etc, regardless of their freuqncy of occurrence in the text. Tokens are nothing but a string of contiguous characters which either lies between the two spaces or it lies between a space and punctuation. For Example: Suppose you have the following string : \"abc_123_defg\", if you split it on basis of underscores \"_\" you obtained three tokens : \"abc\", \"123\" and \"defg\".\n",
    "\n",
    "**What is tokenization?**\n",
    "\n",
    "Tokenization is a process of splitting the sentence or corpus into its smalles unit i.e. \"Tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9a0bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\"\n",
    "R is a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts. It is free and runs on a variety of platforms, including Windows, Unix, and macOS. It provides an unparalleled platform for programming new statistical methods in an easy and straightforward manner. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d13f6259",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_object = TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b60b83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization of the sample corpus\n",
    "corpus_words = blob_object.words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a87be64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['R', 'is', 'a', 'comprehensive', 'statistical', 'and', 'graphical', 'programming', 'language', 'which', 'is', 'fast', 'gaining', 'popularity', 'among', 'data', 'analysts', 'It', 'is', 'free', 'and', 'runs', 'on', 'a', 'variety', 'of', 'platforms', 'including', 'Windows', 'Unix', 'and', 'macOS', 'It', 'provides', 'an', 'unparalleled', 'platform', 'for', 'programming', 'new', 'statistical', 'methods', 'in', 'an', 'easy', 'and', 'straightforward', 'manner'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45a3f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fd6bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_sentences= blob_object.sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5528532a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Sentence(\"\n",
       " R is a comprehensive statistical and graphical programming language, which is fast gaining popularity among data analysts.\"),\n",
       " Sentence(\"It is free and runs on a variety of platforms, including Windows, Unix, and macOS.\"),\n",
       " Sentence(\"It provides an unparalleled platform for programming new statistical methods in an easy and straightforward manner.\")]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b598ee43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f396561",
   "metadata": {},
   "source": [
    "#### Pluralization of words using Textblob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "547f7446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platforms'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('Platform')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bdf342db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Platformss'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import Word\n",
    "w = Word('Platforms')\n",
    "w.pluralize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6343f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "platforms\n",
      "sciences\n",
      "communities\n",
      "etcs\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Great Learning is a great platform to learn data science. \\n It helps community through blogs, Youtube, GLA,etc.\")\n",
    "for word,pos in blob.tags:\n",
    "    if pos == 'NN':\n",
    "        print (word.pluralize())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc3cb03",
   "metadata": {},
   "source": [
    "#### Lemmatization using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9d5ddb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL: Great | LEMMA: Great | STEM: great\n",
      "ORIGINAL: Learning | LEMMA: Learning | STEM: learn\n",
      "ORIGINAL: is | LEMMA: is | STEM: is\n",
      "ORIGINAL: a | LEMMA: a | STEM: a\n",
      "ORIGINAL: great | LEMMA: great | STEM: great\n",
      "ORIGINAL: platform | LEMMA: platform | STEM: platform\n",
      "ORIGINAL: to | LEMMA: to | STEM: to\n",
      "ORIGINAL: learn | LEMMA: learn | STEM: learn\n",
      "ORIGINAL: data | LEMMA: data | STEM: data\n",
      "ORIGINAL: science | LEMMA: science | STEM: scienc\n",
      "ORIGINAL: It | LEMMA: It | STEM: it\n",
      "ORIGINAL: helps | LEMMA: help | STEM: help\n",
      "ORIGINAL: community | LEMMA: community | STEM: commun\n",
      "ORIGINAL: through | LEMMA: through | STEM: through\n",
      "ORIGINAL: blogs | LEMMA: blog | STEM: blog\n",
      "ORIGINAL: Youtube | LEMMA: Youtube | STEM: youtub\n",
      "ORIGINAL: GLA | LEMMA: GLA | STEM: gla\n",
      "ORIGINAL: etc | LEMMA: etc | STEM: etc\n"
     ]
    }
   ],
   "source": [
    "blob = TextBlob(\"Great Learning is a great platform to learn data science. \\n It helps community through blogs, Youtube, GLA,etc.\")\n",
    "words = blob.words\n",
    "\n",
    "for word in words:\n",
    "    print(\"ORIGINAL:\", word, \"| LEMMA:\", word.lemmatize(), \"| STEM:\", word.stem())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1fd4a8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learning'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('learning')\n",
    "w.lemmatize(\"n\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d08a8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'learn'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('learning')\n",
    "w.lemmatize(\"v\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e11b2075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'people'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = Word('peoples')\n",
    "w.lemmatize(\"n\") ## v here represents verb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01024e7c",
   "metadata": {},
   "source": [
    "#### n-gram in Textblob\n",
    "\n",
    "An N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “really good”, “not good”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “not at all”, or “turn off light”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fad2e55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Great Learning is a great platform to learn data science. \n",
       " It helps community through blogs, Youtube, GLA,etc.\")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7fbe2828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great']),\n",
       " WordList(['Learning']),\n",
       " WordList(['is']),\n",
       " WordList(['a']),\n",
       " WordList(['great']),\n",
       " WordList(['platform']),\n",
       " WordList(['to']),\n",
       " WordList(['learn']),\n",
       " WordList(['data']),\n",
       " WordList(['science']),\n",
       " WordList(['It']),\n",
       " WordList(['helps']),\n",
       " WordList(['community']),\n",
       " WordList(['through']),\n",
       " WordList(['blogs']),\n",
       " WordList(['Youtube']),\n",
       " WordList(['GLA']),\n",
       " WordList(['etc'])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67a47ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning']),\n",
       " WordList(['Learning', 'is']),\n",
       " WordList(['is', 'a']),\n",
       " WordList(['a', 'great']),\n",
       " WordList(['great', 'platform']),\n",
       " WordList(['platform', 'to']),\n",
       " WordList(['to', 'learn']),\n",
       " WordList(['learn', 'data']),\n",
       " WordList(['data', 'science']),\n",
       " WordList(['science', 'It']),\n",
       " WordList(['It', 'helps']),\n",
       " WordList(['helps', 'community']),\n",
       " WordList(['community', 'through']),\n",
       " WordList(['through', 'blogs']),\n",
       " WordList(['blogs', 'Youtube']),\n",
       " WordList(['Youtube', 'GLA']),\n",
       " WordList(['GLA', 'etc'])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3346e17e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning', 'is']),\n",
       " WordList(['Learning', 'is', 'a']),\n",
       " WordList(['is', 'a', 'great']),\n",
       " WordList(['a', 'great', 'platform']),\n",
       " WordList(['great', 'platform', 'to']),\n",
       " WordList(['platform', 'to', 'learn']),\n",
       " WordList(['to', 'learn', 'data']),\n",
       " WordList(['learn', 'data', 'science']),\n",
       " WordList(['data', 'science', 'It']),\n",
       " WordList(['science', 'It', 'helps']),\n",
       " WordList(['It', 'helps', 'community']),\n",
       " WordList(['helps', 'community', 'through']),\n",
       " WordList(['community', 'through', 'blogs']),\n",
       " WordList(['through', 'blogs', 'Youtube']),\n",
       " WordList(['blogs', 'Youtube', 'GLA']),\n",
       " WordList(['Youtube', 'GLA', 'etc'])]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e22ce261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['Great', 'Learning', 'is', 'a']),\n",
       " WordList(['Learning', 'is', 'a', 'great']),\n",
       " WordList(['is', 'a', 'great', 'platform']),\n",
       " WordList(['a', 'great', 'platform', 'to']),\n",
       " WordList(['great', 'platform', 'to', 'learn']),\n",
       " WordList(['platform', 'to', 'learn', 'data']),\n",
       " WordList(['to', 'learn', 'data', 'science']),\n",
       " WordList(['learn', 'data', 'science', 'It']),\n",
       " WordList(['data', 'science', 'It', 'helps']),\n",
       " WordList(['science', 'It', 'helps', 'community']),\n",
       " WordList(['It', 'helps', 'community', 'through']),\n",
       " WordList(['helps', 'community', 'through', 'blogs']),\n",
       " WordList(['community', 'through', 'blogs', 'Youtube']),\n",
       " WordList(['through', 'blogs', 'Youtube', 'GLA']),\n",
       " WordList(['blogs', 'Youtube', 'GLA', 'etc'])]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.ngrams(n=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4679995b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
